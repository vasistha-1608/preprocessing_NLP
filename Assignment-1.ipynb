{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e25bd49",
   "metadata": {},
   "source": [
    "# Importing necessary library to load and process csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8dab4ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vasistha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vasistha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\vasistha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "79bb2925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0        label                                          statement  \\\n",
      "1           1    half-true  When did the decline of coal start? It started...   \n",
      "2           2  mostly-true  Hillary Clinton agrees with John McCain \"by vo...   \n",
      "3           3        false  Health care reform legislation is likely to ma...   \n",
      "4           4    half-true  The economic turnaround started at the end of ...   \n",
      "5           5         true  The Chicago Bears have had more starting quart...   \n",
      "6           6  barely-true  Jim Dunnam has not lived in the district he re...   \n",
      "7           7    half-true  I'm the only person on this stage who has work...   \n",
      "8           8    half-true  However, it took $19.5 million in Oregon Lotte...   \n",
      "9           9  mostly-true  Says GOP primary opponents Glenn Grothman and ...   \n",
      "\n",
      "                                       topic                 speaker  \\\n",
      "1         energy,history,job-accomplishments          scott-surovell   \n",
      "2                             foreign-policy            barack-obama   \n",
      "3                                health-care            blog-posting   \n",
      "4                               economy,jobs           charlie-crist   \n",
      "5                                  education               robin-vos   \n",
      "6                       candidates-biography  republican-party-texas   \n",
      "7                                     ethics            barack-obama   \n",
      "8                                       jobs          oregon-lottery   \n",
      "9  energy,message-machine-2014,voting-record           duey-stroebel   \n",
      "\n",
      "                      context   location         party  \\\n",
      "1              State delegate   Virginia      democrat   \n",
      "2                   President   Illinois      democrat   \n",
      "3                         NaN        NaN          none   \n",
      "4                         NaN    Florida      democrat   \n",
      "5  Wisconsin Assembly speaker  Wisconsin    republican   \n",
      "6                         NaN      Texas    republican   \n",
      "7                   President   Illinois      democrat   \n",
      "8                         NaN        NaN  organization   \n",
      "9        State representative  Wisconsin    republican   \n",
      "\n",
      "                                   resource  \n",
      "1                           a floor speech.  \n",
      "2                                    Denver  \n",
      "3                            a news release  \n",
      "4                       an interview on CNN  \n",
      "5                 a an online opinion-piece  \n",
      "6                          a press release.  \n",
      "7  a Democratic debate in Philadelphia, Pa.  \n",
      "8                                a website   \n",
      "9                           an online video  \n"
     ]
    }
   ],
   "source": [
    "fake_news_csv = pd.read_csv('C:/Users/vasistha/Textmining/train.csv')\n",
    "##print first 10 rows of train.csv\n",
    "print(fake_news_csv[1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a08cb64",
   "metadata": {},
   "source": [
    "#### Showing the columns and their data types present in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "589213b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0     int64\n",
      "label         object\n",
      "statement     object\n",
      "topic         object\n",
      "speaker       object\n",
      "context       object\n",
      "location      object\n",
      "party         object\n",
      "resource      object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "## Show the columns present in the csv\n",
    "list_of_columns= fake_news_csv.dtypes\n",
    "print(list_of_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118ecd7b",
   "metadata": {},
   "source": [
    "#### Showing the number of instances present "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "666f4cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10240, 9)\n"
     ]
    }
   ],
   "source": [
    "##Show the number of instances present in the \n",
    "number_of_instances = fake_news_csv.shape\n",
    "print(number_of_instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ecb80a",
   "metadata": {},
   "source": [
    "# Basic Text Tranformation Operations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8f206b",
   "metadata": {},
   "source": [
    "#### Convert text from statement feature to lowecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dfe37255",
   "metadata": {},
   "outputs": [],
   "source": [
    "statement_lower_case_list= fake_news_csv.iloc[0:,2].tolist() \n",
    "##print(statement_lower_case_list)\n",
    "len(statement_lower_case_list)\n",
    "\n",
    "#Starting for each loop to convert the data in the lowercase\n",
    "i =0\n",
    "for l in statement_lower_case_list:\n",
    "    temp= l.lower()\n",
    "    statement_lower_case_list[i]= temp\n",
    "    i=i+1\n",
    "\n",
    "##print(statement_lower_case_list)\n",
    "\n",
    "##print(statement_lower_case_list[45:49])\n",
    "\n",
    "fake_news_csv.iloc[0:,2] = statement_lower_case_list\n",
    "\n",
    "#print(fake_news_csv.iloc[0:5,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72049250",
   "metadata": {},
   "source": [
    "#### Extract quoted text from the statement feature and create new column in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d7365454",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the package for regular expression matching \n",
    "import re\n",
    "\n",
    "statements_quoted_extract_array = fake_news_csv.iloc[1:,2].tolist() \n",
    "quoted_content_list =[[]]\n",
    "i=0\n",
    "for l in statements_quoted_extract_array:\n",
    "    temp_list= re.findall(r'\"(.*?)\"',l)\n",
    "    if(len(temp_list) ==0):\n",
    "        temp_list= ['NA']\n",
    "        \n",
    "        quoted_content_list.append(temp_list)\n",
    "       # quoted_content_list[i] = temp_list[i]\n",
    "    else:\n",
    "        \n",
    "        quoted_content_list.append(temp_list)\n",
    "       # quoted_content_list[i] = temp_list[i]\n",
    "        \n",
    "\n",
    "## Inserting the new column to the data frame first\n",
    "fake_news_csv.insert( loc=len(list_of_columns) ,column=\"quoted_content\",value=quoted_content_list)\n",
    "\n",
    "\n",
    "#Writing the newly updated data changes to csv files\n",
    "fake_news_csv.to_csv('C:/Users/vasistha/Textmining/train.csv')\n",
    "\n",
    "## Note: This has to performed only once or else it will throw an error every time if we try to add the same items\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513abebb",
   "metadata": {},
   "source": [
    "#### Creating a new column for lowercase alpha numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "52fbacfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "alpha_numeric_statements = fake_news_csv.iloc[0:,2].tolist() \n",
    "temp_statement=[]\n",
    "\n",
    "#Iterating over the individual statement to remove special characters and convert the statement to lower case\n",
    "for l in alpha_numeric_statements:\n",
    "    temp=re.sub(r'[^a-zA-Z0-9\\s]+',' ',l)\n",
    "    temp=temp.lower()\n",
    "    temp_statement.append(temp)\n",
    "##print(temp_statement)\n",
    "\n",
    "\n",
    "##Writing this newly updated list to the data frame first\n",
    "fake_news_csv.insert( loc=len(list_of_columns) ,column=\"cleaned_statement\",value=temp_statement)\n",
    "\n",
    "#Writing the newly updated data changes to csv files\n",
    "fake_news_csv.to_csv('C:/Users/vasistha/Textmining/train.csv')\n",
    "\n",
    "## Note: This has to performed only once or else it will throw an error every time if we try to add the same items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757e1673",
   "metadata": {},
   "source": [
    "### Comparision between statement and cleaned statement  Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e6849e",
   "metadata": {},
   "source": [
    "### The first three statements before and after the cleaning becomes\n",
    "\n",
    "#### Statement 1 : Says the Annies list political group supports third-trimester abortions on demand.\n",
    "#### Cleaned Statement 1 : says the annies list political group supports third trimester abortions on demand \n",
    "#### Observation : All the letters are lowered case in cleaned statements when compared to statement 1 and punctuations like ., , , - are removed as part of the special characteter removal processing.\n",
    "\n",
    "#### Statement 2 : When did the decline of coal start? It started when natural gas took off that started to begin in (president George W.) Bush's administration.\n",
    "#### Cleaned Statement 2 : when did the decline of coal start  it started when natural gas took off that started to begin in  president george w  bushs administration \n",
    "#### Observation : All the letters are lowered case in cleaned statements when compared to statement 2 and special characters like paranthases, apostrophes, and commas are removed as part of the special characteter removal processing.\n",
    "\n",
    "#### Statement 3 : Since 2000, nearly 12 million Americans have slipped out of the middle class and into poverty.\n",
    "#### Cleaned Statement 3 : since 2000  nearly 12 million americans have slipped out of the middle class and into poverty \n",
    "#### Observation : All the letters are lowered case in cleaned statements when compared to statement 3 and punctuations like commas and period are removed as part of the special charactetr removal processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f66db2",
   "metadata": {},
   "source": [
    "# Tokenization using two approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e87e226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "## Tokenization using NLTK package for statement column\n",
    "statement_token_list = [[]]\n",
    "statement_list = fake_news_csv.iloc[0:,2].tolist() \n",
    "for l in statement_list: \n",
    "    if l== '':\n",
    "        l='NA'\n",
    "    temp = word_tokenize(l)\n",
    "    statement_token_list.append(temp)\n",
    "    \n",
    "##print(statement_token_list[0:20])\n",
    "\n",
    "##Tokenization using NLTK package for cleaned statement column\n",
    "\n",
    "clean_statement_token_list = [[]]\n",
    "cleaned_statement_list = fake_news_csv.iloc[0:,9].tolist() \n",
    "\n",
    "for l in cleaned_statement_list:\n",
    "    if l== '':\n",
    "        l='NA'\n",
    "    temp = word_tokenize(l)\n",
    "    clean_statement_token_list.append(temp)\n",
    "    \n",
    "##print(clean_statement_token_list[0:20])\n",
    "\n",
    "#Writing these two new columns with dataframes first \n",
    "fake_news_csv.insert( loc=len(list_of_columns) ,column=\"tokenized_statement\",value=statement_token_list[0:10240])\n",
    "fake_news_csv.insert( loc=len(list_of_columns) ,column=\"tokenized_cleaned_statement\",value=clean_statement_token_list[0:10240])\n",
    "\n",
    "##Updating this to the csv file\n",
    "fake_news_csv.to_csv('C:/Users/vasistha/Textmining/train.csv')\n",
    "\n",
    "## Note: This has to performed only once or else it will throw an error every time if we try to add the same items\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "014c481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_csv = fake_news_csv.drop('tokenized_statement', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1813823",
   "metadata": {},
   "source": [
    "#### The main difference between tokenizing a normal statement and a statement which has already been cleaned in terms of special characters removal and converting all letters to lower cases is that the tokenization takes more space and time to process a normal statement and would not be efficient when it comes to cleaning data with large columns and and large number of characters. \n",
    "\n",
    "#### Cleaned tokenized statement in general keeps more relevant and useful data on which we can perform text mining algorithms in a more efficient manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9df439d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "says say 100\n",
      "the the 90\n",
      "annies annie 92\n",
      "list list 100\n",
      "political political 84\n",
      "group group 92\n",
      "supports support 100\n",
      "third third 84\n",
      "trimester trimester 92\n",
      "abortions abortion 92\n",
      "on on 85\n",
      "demand demand 92\n",
      "when when 86\n",
      "did do 87\n",
      "the the 90\n",
      "decline decline 92\n",
      "of of 85\n",
      "coal coal 92\n",
      "start start 92\n",
      "it it 95\n",
      "started start 100\n",
      "when when 86\n",
      "natural natural 84\n",
      "gas gas 92\n",
      "took take 100\n",
      "off off 85\n",
      "that that 90\n",
      "started start 100\n",
      "to to 94\n",
      "begin begin 100\n",
      "in in 85\n",
      "president president 92\n",
      "george george 100\n",
      "w w 85\n",
      "bushs bushs 84\n",
      "administration administration 92\n",
      "['says', 'the', 'annies', 'list', 'political', 'group', 'supports', 'third-trimester', 'abortions', 'on', 'demand', '.']\n",
      "['when', 'did', 'the', 'decline', 'of', 'coal', 'start', '?', 'it', 'started', 'when', 'natural', 'gas', 'took', 'off', 'that', 'started', 'to', 'begin', 'in', '(', 'president', 'george', 'w.', ')', 'bushs', 'administration', '.']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "## Tokenization using spacy package on the cleaned_statement_column\n",
    "\n",
    "spacy_tokenized_statement=[[]]\n",
    "spacy_cleaned_statement_list = fake_news_csv.iloc[1:3,9].tolist() \n",
    "##print(spacy_cleaned_statement_list)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "for l in spacy_cleaned_statement_list:\n",
    "    temp = nlp(' '.join(l))\n",
    "    for token in temp: \n",
    "        print(token.text, token.lemma_,token.pos)\n",
    "\n",
    "for l in statement_token_list[1:3]:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046e1dd1",
   "metadata": {},
   "source": [
    "#### The spacy package which I used for tokenizaiton went to a deeper level of data pre-processing which involved identitfying root words, categorizing which parts of speech they belong to. The output above shows obvious differences between tokenization using spacy and tokenization using NLTK. \n",
    "\n",
    "#### Programatically speaking,  the word_tokenize in NLTK method just identifies white space as the delimiter or any other custom delimeter we provide, to seperate the tokens in a sentence and it's output is a list of tokens in the sentence. It perfoms simple preprocessing. Unlike, the NLTK package, spacy returns a list of object and each object is a token. That means each and every token will have an attribute like text, lemma_,pos_ etc and that attribute will hold a certain value which tells us about a certain aspect of a token.\n",
    "\n",
    "#### Performance wise, as the spacy processing data at so many deep levels, it tends to take more time and computational resources. But it really depends on the use case of the problem depending on which we will select our package to pre process the data. If we just need it for simple classification then NLTK will be suitable whereas if the application is that of sentiment analysis, then a granular level of understanding of language is required where spacy package will be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "52c76574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vasistha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afba8cb5",
   "metadata": {},
   "source": [
    "# Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "18418716",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing stop words from the NLTK package \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "##print(stop_words)\n",
    "\n",
    "stopword_removed_statement_list=[[]]\n",
    "\n",
    "for l in clean_statement_token_list:\n",
    "    temp=[]\n",
    "    for words in l :\n",
    "        if(words not in stop_words):\n",
    "            temp.append(words)\n",
    "    stopword_removed_statement_list.append(temp)\n",
    "##print(stopword_removed_statement_list[1:10])\n",
    "\n",
    "##Writing this newly updated list to the data frame first\n",
    "fake_news_csv.insert( loc=len(list_of_columns) ,column=\"stopword_removed_statement\",\n",
    "                     value=stopword_removed_statement_list[0:10240])\n",
    "\n",
    "##Updating this to the csv file\n",
    "fake_news_csv.to_csv('C:/Users/vasistha/Textmining/train.csv')\n",
    "\n",
    "## Note: This has to performed only once or else it will throw an error every time if we try to add the same items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ff5030e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_csv = fake_news_csv.drop('stopword_removed_statement', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33d4364",
   "metadata": {},
   "source": [
    "# Stemming the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "61476d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming the data set given to us using the NLTK package \n",
    "\n",
    "from nltk.stem import PorterStemmer \n",
    "ps = PorterStemmer() \n",
    "\n",
    "stemmed_statements_list=[[]]\n",
    "\n",
    "for l in stopword_removed_statement_list: \n",
    "    temp=[]\n",
    "    for words in l:\n",
    "        words = ps.stem(words)\n",
    "        temp.append(words)\n",
    "    stemmed_statements_list.append(temp)\n",
    "##print(stemmed_statements_list[1:10])\n",
    "\n",
    "##Writing this newly updated list to the data frame first\n",
    "fake_news_csv.insert( loc=len(list_of_columns) ,column=\"stemmed_statement\",\n",
    "                     value=stemmed_statements_list[0:10240])\n",
    "\n",
    "##Updating this to the csv file\n",
    "fake_news_csv.to_csv('C:/Users/vasistha/Textmining/train.csv')\n",
    "\n",
    "\n",
    "\n",
    "## Note: This has to performed only once or else it will throw an error every time if we try to add the same items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c911d832",
   "metadata": {},
   "source": [
    "### Comparision between the stop word removal list to stemmed list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "09bb2150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['economic', 'turnaround', 'started', 'end', 'term'], ['chicago', 'bears', 'starting', 'quarterbacks', 'last', '10', 'years', 'total', 'number', 'tenured', 'uw', 'faculty', 'fired', 'last', 'two', 'decades'], ['jim', 'dunnam', 'lived', 'district', 'represents', 'years'], ['person', 'stage', 'worked', 'actively', 'last', 'year', 'passing', 'along', 'russ', 'feingold', 'toughest', 'ethics', 'reform', 'since', 'watergate']]\n",
      "[['econom', 'turnaround', 'start', 'end', 'term'], ['chicago', 'bear', 'start', 'quarterback', 'last', '10', 'year', 'total', 'number', 'tenur', 'uw', 'faculti', 'fire', 'last', 'two', 'decad'], ['jim', 'dunnam', 'live', 'district', 'repres', 'year'], ['person', 'stage', 'work', 'activ', 'last', 'year', 'pass', 'along', 'russ', 'feingold', 'toughest', 'ethic', 'reform', 'sinc', 'waterg']]\n"
     ]
    }
   ],
   "source": [
    "print(stopword_removed_statement_list[6:10])\n",
    "\n",
    "print(stemmed_statements_list[7:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "593be901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['person', 'stage', 'work', 'activ', 'last', 'year', 'pass', 'along', 'russ', 'feingold', 'toughest', 'ethic', 'reform', 'sinc', 'waterg'], ['howev', 'took', '19', '5', 'million', 'oregon', 'lotteri', 'fund', 'port', 'newport', 'eventu', 'land', 'new', 'noaa', 'marin', 'oper', 'center', 'pacif'], ['say', 'gop', 'primari', 'oppon', 'glenn', 'grothman', 'joe', 'leibham', 'cast', 'compromis', 'vote', 'cost', '788', 'million', 'higher', 'electr', 'cost'], ['first', 'time', 'histori', 'share', 'nation', 'popular', 'vote', 'margin', 'smaller', 'latino', 'vote', 'margin'], ['sinc', '2000', 'near', '12', 'million', 'american', 'slip', 'middl', 'class', 'poverti'], ['mitt', 'romney', 'governor', 'massachusett', 'didnt', 'slow', 'rate', 'growth', 'govern', 'actual', 'cut'], ['economi', 'bled', '24', 'billion', 'due', 'govern', 'shutdown'], ['afford', 'care', 'act', 'alreadi', 'sens', 'waiv', 'otherwis', 'suspend'], ['last', 'elect', 'novemb', '63', 'percent', 'american', 'peopl', 'chose', 'vote', '80', 'percent', 'young', 'peopl', '75', 'percent', 'low', 'incom', 'worker', 'chose', 'vote'], ['mccain', 'oppos', 'requir', 'govern', 'buy', 'american', 'made', 'motorcycl', 'said', 'buy', 'american', 'provis', 'quot', 'disgrac']]\n",
      "[['person', 'stage', 'work', 'activ', 'last', 'year', 'pass', 'along', 'russ', 'feingold', 'toughest', 'ethic', 'reform', 'sinc', 'waterg'], ['howev', 'took', '19', '5', 'million', 'oregon', 'lotteri', 'fund', 'port', 'newport', 'eventu', 'land', 'new', 'noaa', 'marin', 'oper', 'center', 'pacif'], ['say', 'gop', 'primari', 'oppon', 'glenn', 'grothman', 'joe', 'leibham', 'cast', 'compromis', 'vote', 'cost', '788', 'million', 'higher', 'electr', 'cost'], ['first', 'time', 'histori', 'share', 'nation', 'popular', 'vote', 'margin', 'smaller', 'latino', 'vote', 'margin'], ['sinc', '2000', 'nearli', '12', 'million', 'american', 'slip', 'middl', 'class', 'poverti'], ['mitt', 'romney', 'governor', 'massachusett', 'didnt', 'slow', 'rate', 'growth', 'govern', 'actual', 'cut'], ['economi', 'bled', '24', 'billion', 'due', 'govern', 'shutdown'], ['afford', 'care', 'act', 'alreadi', 'sens', 'waiv', 'otherwis', 'suspend'], ['last', 'elect', 'novemb', '63', 'percent', 'american', 'peopl', 'chose', 'vote', '80', 'percent', 'young', 'peopl', '75', 'percent', 'low', 'incom', 'worker', 'chose', 'vote'], ['mccain', 'oppos', 'requir', 'govern', 'buy', 'american', 'made', 'motorcycl', 'said', 'buy', 'american', 'provis', 'quot', 'disgrac']]\n"
     ]
    }
   ],
   "source": [
    "## Using Snowball Stemmer to stem the data  \n",
    "\n",
    "from nltk.stem import SnowballStemmer \n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "snowball_stemmed_list = [[]]\n",
    "for l in stopword_removed_statement_list: \n",
    "    temp=[]\n",
    "    for words in l:\n",
    "        words = stemmer.stem(words)\n",
    "        temp.append(words)\n",
    "    snowball_stemmed_list.append(temp)\n",
    "print(snowball_stemmed_list[10:20])\n",
    "\n",
    "print(stemmed_statements_list[10:20])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ddda88",
   "metadata": {},
   "source": [
    "### Similarities and differences between Porter Stemming and Snowball stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e9ffa9",
   "metadata": {},
   "source": [
    "According to the website https://www.analyticsvidhya.com/blog/2021/11/an-introduction-to-stemming-in-natural-language-processing/\n",
    "\n",
    "#### The main difference betweeen the Porter stemming and snowball stemming is that snowball stemming is more precise and a bit more logical than porter stemming. It also states that snowball stemming is faster in doing the stemming process than the porter stemming and thus is more widely used.\n",
    "\n",
    "#### When it comes to similarities, both the techniques are quite similar as to what they produce in general. The root word is recognised and shown as the output. For both the techniques it can be said the the actual word might mean something entirely different than what remains after the stemming process.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b408c815",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:textmining] *",
   "language": "python",
   "name": "conda-env-textmining-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
